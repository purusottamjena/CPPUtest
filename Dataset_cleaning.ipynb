{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNfQDNtkmz7mvAzZgBF9xrH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purusottamjena/CPPUtest/blob/main/Dataset_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "%pip install --upgrade datasets pandas numpy\n",
        "\n",
        "\n",
        "# Authenticate with Hugging Face Hub\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "zWXghVHPa_J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clVWu79paaGL"
      },
      "outputs": [],
      "source": [
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# %%\n",
        "from huggingface_hub import notebook_login\n",
        "# %pip install --upgrade datasets pandas numpy\n",
        "\n",
        "\n",
        "# Authenticate with Hugging Face Hub\n",
        "notebook_login()\n",
        "# %%\n",
        "\n",
        "\n",
        "# Embedded Unit Test Dataset Cleaning Script for Google Colab\n",
        "# This script cleans the Hugging Face dataset: athrv/Embedded_unit_test\n",
        "\n",
        "# Install required packages\n",
        "# !pip install --upgrade datasets pandas numpy  # Upgrade datasets to the latest version\n",
        "%pip install --upgrade datasets==2.14.6 pandas numpy scipy\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "# Embedded Unit Test Dataset Cleaning Script for Google Colab\n",
        "# This script cleans the Hugging Face dataset: athrv/Embedded_unit_test\n",
        "\n",
        "# # Install required packages\n",
        "# !pip install datasets pandas numpy\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "from typing import Optional, List, Dict\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class EmbeddedDatasetCleaner:\n",
        "    \"\"\"\n",
        "    A comprehensive cleaner for the Embedded Unit Test dataset\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.original_data = None\n",
        "        self.cleaned_data = None\n",
        "        self.cleaning_stats = {\n",
        "            'original_rows': 0,\n",
        "            'rows_after_null_removal': 0,\n",
        "            'rows_after_minimal_content_removal': 0,\n",
        "            'rows_after_duplicate_removal': 0,\n",
        "            'comments_removed': 0,\n",
        "            'final_rows': 0\n",
        "        }\n",
        "\n",
        "    def load_dataset(self):\n",
        "        \"\"\"Load the dataset from Hugging Face\"\"\"\n",
        "        print(\"Loading dataset from Hugging Face...\")\n",
        "        dataset = load_dataset(\"athrv/Embedded3\")\n",
        "        self.original_data = dataset['train'].to_pandas()\n",
        "        self.cleaning_stats['original_rows'] = len(self.original_data)\n",
        "        print(f\"✅ Dataset loaded: {len(self.original_data)} rows\")\n",
        "        return self.original_data.copy()\n",
        "\n",
        "    def remove_comments_from_code(self, code_text: str) -> str:\n",
        "        \"\"\"\n",
        "        Remove various types of comments from code while preserving string literals\n",
        "        \"\"\"\n",
        "        if pd.isna(code_text) or code_text == \"Not Found\" or code_text == \"null\":\n",
        "            return code_text\n",
        "\n",
        "        # Remove single-line comments (// style)\n",
        "        # This regex removes // comments but preserves them if they're in string literals\n",
        "        code_text = re.sub(r'^\\s*//.*$', '', code_text, flags=re.MULTILINE)\n",
        "\n",
        "        # Remove multi-line comments (/* */ style)\n",
        "        code_text = re.sub(r'/\\*.*?\\*/', '', code_text, flags=re.DOTALL)\n",
        "\n",
        "        # Remove commented code blocks (especially //NOSONAR marked blocks)\n",
        "        # Remove lines that are clearly commented out code\n",
        "        lines = code_text.split('\\n')\n",
        "        cleaned_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            stripped = line.strip()\n",
        "            # Skip if line is just a comment or empty\n",
        "            if (stripped.startswith('//') or\n",
        "                stripped.startswith('/*') or\n",
        "                stripped.startswith('*') or\n",
        "                stripped == '' or\n",
        "                '//NOSONAR' in stripped):\n",
        "                continue\n",
        "            cleaned_lines.append(line)\n",
        "\n",
        "        # Join back and clean up extra whitespace\n",
        "        result = '\\n'.join(cleaned_lines)\n",
        "\n",
        "        # Remove excessive newlines (more than 2 consecutive)\n",
        "        result = re.sub(r'\\n{3,}', '\\n\\n', result)\n",
        "\n",
        "        # Clean up leading/trailing whitespace\n",
        "        result = result.strip()\n",
        "\n",
        "        return result\n",
        "\n",
        "    def is_minimal_content(self, code_text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Check if code content is minimal/empty and should be removed\n",
        "        \"\"\"\n",
        "        if pd.isna(code_text) or code_text == \"Not Found\" or code_text == \"null\":\n",
        "            return True\n",
        "\n",
        "        # Clean the text for analysis\n",
        "        cleaned = self.remove_comments_from_code(code_text)\n",
        "        cleaned = re.sub(r'\\s+', ' ', cleaned).strip()\n",
        "\n",
        "        # Consider minimal if:\n",
        "        # 1. Empty after cleaning\n",
        "        # 2. Only contains pragma once\n",
        "        # 3. Only contains include statements\n",
        "        # 4. Very short (less than 50 characters of actual code)\n",
        "\n",
        "        if len(cleaned) == 0:\n",
        "            return True\n",
        "\n",
        "        if cleaned in ['#pragma once', '#pragma once;']:\n",
        "            return True\n",
        "\n",
        "        # Count non-whitespace, non-preprocessor characters\n",
        "        code_content = re.sub(r'#.*$', '', cleaned, flags=re.MULTILINE)\n",
        "        code_content = re.sub(r'\\s+', '', code_content)\n",
        "\n",
        "        return len(code_content) < 20  # Very minimal actual code\n",
        "\n",
        "    def remove_null_and_not_found(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Remove rows where key code columns are null, 'Not Found', or 'null'\n",
        "        \"\"\"\n",
        "        print(\"🧹 Removing rows with null/not found content...\")\n",
        "\n",
        "        # Columns that should have meaningful content - UPDATED COLUMN NAMES\n",
        "        key_columns = ['Code1', 'Unit Test (.cpp file)']\n",
        "\n",
        "        initial_count = len(df)\n",
        "\n",
        "        # Remove rows where ALL key columns are null/not found\n",
        "        indices_to_remove = []\n",
        "        for idx, row in df.iterrows():\n",
        "            all_empty = True\n",
        "            for col in key_columns:\n",
        "                if col in df.columns:\n",
        "                    if (not pd.isna(row[col]) and\n",
        "                        row[col] not in ['Not Found', 'null', None, '']):\n",
        "                        all_empty = False\n",
        "                        break\n",
        "\n",
        "            if all_empty:\n",
        "                indices_to_remove.append(idx)\n",
        "\n",
        "        df = df.drop(indices_to_remove)\n",
        "\n",
        "        final_count = len(df)\n",
        "        removed = initial_count - final_count\n",
        "        self.cleaning_stats['rows_after_null_removal'] = final_count\n",
        "\n",
        "        print(f\"   Removed {removed} rows with no meaningful content\")\n",
        "        print(f\"   Remaining: {final_count} rows\")\n",
        "        return df # Return the modified dataframe\n",
        "\n",
        "    def clean_dataset_gentle(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Gentle cleaning pipeline that preserves more data\n",
        "        \"\"\"\n",
        "        print(\"\\n🚀 Starting GENTLE dataset cleaning pipeline...\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Load dataset\n",
        "        df = self.cleaned_data if self.cleaned_data is not None else self.original_data.copy()\n",
        "\n",
        "        # Step 1: Only remove completely empty rows\n",
        "        df = self.remove_completely_empty_rows(df)\n",
        "\n",
        "        # Step 2: Clean comments but preserve structure\n",
        "        df = self.clean_code_content(df)\n",
        "\n",
        "        # Step 3: Only remove exact duplicates\n",
        "        df = self.remove_exact_duplicates(df)\n",
        "\n",
        "        self.cleaned_data = df\n",
        "        self.cleaning_stats['final_rows'] = len(df)\n",
        "\n",
        "        print(\"\\n✅ Gentle cleaning completed!\")\n",
        "        self.print_cleaning_summary()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def remove_completely_empty_rows(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Only remove rows that are completely empty in all important columns\n",
        "        \"\"\"\n",
        "        print(\"🧹 Removing completely empty rows...\")\n",
        "\n",
        "        initial_count = len(df)\n",
        "\n",
        "        # Only remove if ALL important columns are empty/null/not found\n",
        "        important_cols = ['Code1', 'Unit Test (.cpp file)', 'Base File Name']\n",
        "\n",
        "        indices_to_remove = []\n",
        "        for idx, row in df.iterrows():\n",
        "            completely_empty = True\n",
        "            for col in important_cols:\n",
        "                if col in df.columns:\n",
        "                    val = row[col]\n",
        "                    if (pd.notna(val) and\n",
        "                        val not in ['Not Found', 'null', None, ''] and\n",
        "                        len(str(val).strip()) > 5):  # At least some content\n",
        "                        completely_empty = False\n",
        "                        break\n",
        "\n",
        "            if completely_empty:\n",
        "                indices_to_remove.append(idx)\n",
        "\n",
        "        df = df.drop(indices_to_remove)\n",
        "\n",
        "        final_count = len(df)\n",
        "        removed = initial_count - final_count\n",
        "        self.cleaning_stats['rows_after_null_removal'] = final_count\n",
        "        self.cleaning_stats['rows_after_minimal_content_removal'] = final_count\n",
        "\n",
        "        print(f\"   Removed {removed} completely empty rows\")\n",
        "        print(f\"   Remaining: {final_count} rows\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def remove_exact_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Remove only exact duplicates, not similar content\n",
        "        \"\"\"\n",
        "        print(\"🧹 Removing exact duplicates...\")\n",
        "\n",
        "        initial_count = len(df)\n",
        "\n",
        "        # Use pandas built-in duplicate removal on key columns\n",
        "        key_cols = []\n",
        "        for col in ['Code1', 'Unit Test (.cpp file)', 'Base File Name']:\n",
        "            if col in df.columns:\n",
        "                key_cols.append(col)\n",
        "\n",
        "        if key_cols:\n",
        "            df = df.drop_duplicates(subset=key_cols, keep='first')\n",
        "\n",
        "        final_count = len(df)\n",
        "        removed = initial_count - final_count\n",
        "        self.cleaning_stats['rows_after_duplicate_removal'] = final_count\n",
        "\n",
        "        print(f\"   Removed {removed} exact duplicate rows\")\n",
        "        print(f\"   Remaining: {final_count} rows\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def remove_minimal_content_files(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Remove files that have only minimal content (like just #pragma once)\n",
        "        \"\"\"\n",
        "        print(\"🧹 Removing files with minimal content...\")\n",
        "\n",
        "        initial_count = len(df)\n",
        "\n",
        "        # Check each row for minimal content - UPDATED COLUMN NAMES\n",
        "        indices_to_remove = []\n",
        "\n",
        "        for idx, row in df.iterrows():\n",
        "            code_cols = ['Code1', 'Unit Test (.cpp file)']\n",
        "\n",
        "            # Count how many columns have substantial content\n",
        "            substantial_content_count = 0\n",
        "\n",
        "            for col in code_cols:\n",
        "                if col in df.columns:\n",
        "                    if (not pd.isna(row[col]) and\n",
        "                        row[col] not in ['Not Found', 'null', None, ''] and\n",
        "                        not self.is_minimal_content(row[col])):\n",
        "                        substantial_content_count += 1\n",
        "\n",
        "            # Remove if no substantial content in any column\n",
        "            if substantial_content_count == 0:\n",
        "                indices_to_remove.append(idx)\n",
        "\n",
        "        df = df.drop(indices_to_remove)\n",
        "\n",
        "        final_count = len(df)\n",
        "        removed = initial_count - final_count\n",
        "        self.cleaning_stats['rows_after_minimal_content_removal'] = final_count\n",
        "\n",
        "        print(f\"   Removed {removed} rows with minimal content\")\n",
        "        print(f\"   Remaining: {final_count} rows\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def clean_code_content(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Clean comments from all code columns\n",
        "        \"\"\"\n",
        "        print(\"🧹 Removing comments from code content...\")\n",
        "\n",
        "        # UPDATED COLUMN NAMES\n",
        "        code_columns = ['Code1', 'Unit Test (.cpp file)', 'CMakeLists']\n",
        "        comment_count = 0\n",
        "\n",
        "        for col in code_columns:\n",
        "            if col in df.columns:\n",
        "                print(f\"   Cleaning comments from '{col}'...\")\n",
        "                original_lengths = df[col].astype(str).str.len().sum()\n",
        "                df[col] = df[col].apply(self.remove_comments_from_code)\n",
        "                cleaned_lengths = df[col].astype(str).str.len().sum()\n",
        "                removed = original_lengths - cleaned_lengths\n",
        "                comment_count += removed\n",
        "                print(f\"   Removed {removed:,} characters from '{col}'\")\n",
        "            else:\n",
        "                print(f\"   Warning: Code column '{col}' not found for cleaning.\")\n",
        "\n",
        "        self.cleaning_stats['comments_removed'] = comment_count\n",
        "        print(f\"   Total removed: {comment_count:,} characters of comments\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def remove_duplicate_cmake_content(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Remove duplicate CMakeLists content and replace with simplified version\n",
        "        \"\"\"\n",
        "        print(\"🧹 Cleaning duplicate CMakeLists content...\")\n",
        "\n",
        "        if 'CMakeLists' in df.columns:\n",
        "            # Find the most common CMakeLists content\n",
        "            cmake_counts = df['CMakeLists'].value_counts()\n",
        "\n",
        "            if len(cmake_counts) > 0:\n",
        "                most_common = cmake_counts.index[0]\n",
        "                count = cmake_counts.iloc[0]\n",
        "\n",
        "                print(f\"   Most common CMakeLists content appears {count} times\")\n",
        "\n",
        "                # Replace overly long CMakeLists with a simplified version\n",
        "                simplified_cmake = \"\"\"cmake_minimum_required(VERSION 3.16)\n",
        "project(embedded_project)\n",
        "enable_testing()\n",
        "add_executable(tests test_sources)\n",
        "target_link_libraries(tests)\"\"\"\n",
        "\n",
        "                # Replace long CMakeLists content\n",
        "                df.loc[df['CMakeLists'].str.len() > 500, 'CMakeLists'] = simplified_cmake\n",
        "\n",
        "        return df\n",
        "\n",
        "    def remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Remove duplicate rows based on code content\n",
        "        \"\"\"\n",
        "        print(\"🧹 Removing duplicate rows...\")\n",
        "\n",
        "        initial_count = len(df)\n",
        "\n",
        "        # Create a hash of the main code content for duplicate detection - UPDATED COLUMN NAMES\n",
        "        hash_components = []\n",
        "\n",
        "        if 'Code1' in df.columns:\n",
        "            hash_components.append(df['Code1'].astype(str))\n",
        "        if 'Unit Test (.cpp file)' in df.columns:\n",
        "            hash_components.append(df['Unit Test (.cpp file)'].astype(str))\n",
        "        if 'Base File Name' in df.columns:\n",
        "            hash_components.append(df['Base File Name'].astype(str))\n",
        "\n",
        "        if hash_components:\n",
        "            df['content_hash'] = pd.concat(hash_components, axis=1).apply(\n",
        "                lambda x: hash(tuple(x)), axis=1\n",
        "            )\n",
        "\n",
        "            # Remove duplicates based on content hash\n",
        "            df = df.drop_duplicates(subset=['content_hash'])\n",
        "            df = df.drop('content_hash', axis=1)\n",
        "\n",
        "        final_count = len(df)\n",
        "        removed = initial_count - final_count\n",
        "        self.cleaning_stats['rows_after_duplicate_removal'] = final_count\n",
        "\n",
        "        print(f\"   Removed {removed} duplicate rows\")\n",
        "        print(f\"   Remaining: {final_count} rows\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def clean_dataset(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Main cleaning pipeline\n",
        "        \"\"\"\n",
        "        print(\"🚀 Starting dataset cleaning pipeline...\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        # Load dataset\n",
        "        df = self.load_dataset()\n",
        "\n",
        "        # Step 1: Remove null/not found entries\n",
        "        df = self.remove_null_and_not_found(df)\n",
        "\n",
        "        # Step 2: Remove minimal content files\n",
        "        df = self.remove_minimal_content_files(df)\n",
        "\n",
        "        # Step 3: Clean code content (remove comments)\n",
        "        df = self.clean_code_content(df)\n",
        "\n",
        "        # Step 4: Clean CMakeLists duplicates\n",
        "        df = self.remove_duplicate_cmake_content(df)\n",
        "\n",
        "        # Step 5: Remove duplicates\n",
        "        df = self.remove_duplicates(df)\n",
        "\n",
        "        self.cleaned_data = df\n",
        "        self.cleaning_stats['final_rows'] = len(df)\n",
        "\n",
        "        print(\"\\n✅ Cleaning completed!\")\n",
        "        self.print_cleaning_summary()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def print_cleaning_summary(self):\n",
        "        \"\"\"\n",
        "        Print a summary of the cleaning process\n",
        "        \"\"\"\n",
        "        print(\"\\n📊 CLEANING SUMMARY\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"Original rows:              {self.cleaning_stats['original_rows']:,}\")\n",
        "        print(f\"After null removal:         {self.cleaning_stats['rows_after_null_removal']:,}\")\n",
        "        print(f\"After minimal content:      {self.cleaning_stats['rows_after_minimal_content_removal']:,}\")\n",
        "        print(f\"After duplicate removal:    {self.cleaning_stats['rows_after_duplicate_removal']:,}\")\n",
        "        print(f\"Final rows:                 {self.cleaning_stats['final_rows']:,}\")\n",
        "        print(f\"Comments removed (chars):   {self.cleaning_stats['comments_removed']:,}\")\n",
        "        print(f\"Total reduction:            {self.cleaning_stats['original_rows'] - self.cleaning_stats['final_rows']:,} rows\")\n",
        "        print(f\"Reduction percentage:       {((self.cleaning_stats['original_rows'] - self.cleaning_stats['final_rows']) / self.cleaning_stats['original_rows']) * 100:.1f}%\")\n",
        "\n",
        "    def analyze_cleaned_data(self):\n",
        "        \"\"\"\n",
        "        Analyze the cleaned dataset\n",
        "        \"\"\"\n",
        "        if self.cleaned_data is None:\n",
        "            print(\"❌ No cleaned data available. Run clean_dataset() first.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\n📈 CLEANED DATASET ANALYSIS\")\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "        df = self.cleaned_data\n",
        "\n",
        "        # Basic statistics\n",
        "        print(f\"Total cleaned rows: {len(df)}\")\n",
        "        print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "        # Category distribution\n",
        "        if 'Category' in df.columns:\n",
        "            print(\"\\n📂 Category Distribution:\")\n",
        "            category_counts = df['Category'].value_counts()\n",
        "            for cat, count in category_counts.items():\n",
        "                print(f\"   {cat}: {count}\")\n",
        "\n",
        "        # Language distribution\n",
        "        if 'Language' in df.columns:\n",
        "            print(f\"\\n💻 Language: {df['Language'].value_counts().to_dict()}\")\n",
        "\n",
        "        # Content length statistics - UPDATED COLUMN NAMES\n",
        "        code_cols = ['Code1', 'Unit Test (.cpp file)']\n",
        "        for col in code_cols:\n",
        "            if col in df.columns:\n",
        "                non_null = df[col].dropna()\n",
        "                non_null = non_null[non_null != 'Not Found']\n",
        "                non_null = non_null[non_null != 'null']\n",
        "                non_null = non_null[non_null != '']\n",
        "\n",
        "                if len(non_null) > 0:\n",
        "                    lengths = non_null.str.len()\n",
        "                    print(f\"\\n📏 {col} Length Stats:\")\n",
        "                    print(f\"   Count: {len(non_null)}\")\n",
        "                    print(f\"   Mean length: {lengths.mean():.0f} chars\")\n",
        "                    print(f\"   Median length: {lengths.median():.0f} chars\")\n",
        "                    print(f\"   Max length: {lengths.max():,} chars\")\n",
        "\n",
        "    def save_cleaned_data(self, filename: str = \"cleaned_embedded_dataset.csv\"):\n",
        "        \"\"\"\n",
        "        Save the cleaned dataset to CSV\n",
        "        \"\"\"\n",
        "        if self.cleaned_data is None:\n",
        "            print(\"❌ No cleaned data available. Run clean_dataset() first.\")\n",
        "            return\n",
        "\n",
        "        self.cleaned_data.to_csv(filename, index=False)\n",
        "        print(f\"💾 Cleaned dataset saved to: {filename}\")\n",
        "\n",
        "        # Also save a sample for inspection\n",
        "        sample_file = f\"sample_{filename}\"\n",
        "        self.cleaned_data.head(10).to_csv(sample_file, index=False)\n",
        "        print(f\"📄 Sample (10 rows) saved to: {sample_file}\")\n",
        "\n",
        "    def create_visualizations(self):\n",
        "        \"\"\"\n",
        "        Create visualizations of the cleaning results\n",
        "        \"\"\"\n",
        "        if self.cleaned_data is None:\n",
        "            print(\"❌ No cleaned data available. Run clean_dataset() first.\")\n",
        "            return\n",
        "\n",
        "        plt.figure(figsize=(15, 10))\n",
        "\n",
        "        # Subplot 1: Cleaning pipeline results\n",
        "        plt.subplot(2, 3, 1)\n",
        "        steps = ['Original', 'After Null\\nRemoval', 'After Minimal\\nContent', 'After\\nDuplicates', 'Final']\n",
        "        counts = [\n",
        "            self.cleaning_stats['original_rows'],\n",
        "            self.cleaning_stats['rows_after_null_removal'],\n",
        "            self.cleaning_stats['rows_after_minimal_content_removal'],\n",
        "            self.cleaning_stats['rows_after_duplicate_removal'],\n",
        "            self.cleaning_stats['final_rows']\n",
        "        ]\n",
        "\n",
        "        plt.bar(steps, counts, color=['red', 'orange', 'yellow', 'lightgreen', 'green'])\n",
        "        plt.title('Dataset Cleaning Pipeline')\n",
        "        plt.ylabel('Number of Rows')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # Subplot 2: Category distribution\n",
        "        plt.subplot(2, 3, 2)\n",
        "        if 'Category' in self.cleaned_data.columns:\n",
        "            category_counts = self.cleaned_data['Category'].value_counts()\n",
        "            plt.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%')\n",
        "            plt.title('File Categories')\n",
        "\n",
        "        # Subplot 3: Code length distribution - UPDATED COLUMN NAMES\n",
        "        plt.subplot(2, 3, 3)\n",
        "        code_cols = ['Code1', 'Unit Test (.cpp file)']\n",
        "        lengths = []\n",
        "        labels = []\n",
        "\n",
        "        for col in code_cols:\n",
        "            if col in self.cleaned_data.columns:\n",
        "                non_null = self.cleaned_data[col].dropna()\n",
        "                non_null = non_null[non_null != 'Not Found']\n",
        "                non_null = non_null[non_null != 'null']\n",
        "                non_null = non_null[non_null != '']\n",
        "\n",
        "                if len(non_null) > 0:\n",
        "                    lengths.extend(non_null.str.len().tolist())\n",
        "                    labels.extend([col.split()[0]] * len(non_null))\n",
        "\n",
        "        if lengths:\n",
        "            plt.hist(lengths, bins=30, alpha=0.7)\n",
        "            plt.title('Code Length Distribution')\n",
        "            plt.xlabel('Characters')\n",
        "            plt.ylabel('Frequency')\n",
        "            plt.yscale('log')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Additional utility functions for further analysis\n",
        "# Moved these functions before the main() function call\n",
        "def examine_dataset_structure(df):\n",
        "    \"\"\"\n",
        "    Detailed examination of the dataset structure\n",
        "    \"\"\"\n",
        "    print(f\"\\n🔬 DETAILED DATASET EXAMINATION\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Show first few rows\n",
        "    print(\"📋 First 3 rows:\")\n",
        "    for i in range(min(3, len(df))):\n",
        "        print(f\"\\nRow {i+1}:\")\n",
        "        row = df.iloc[i]\n",
        "        for col in df.columns:\n",
        "            val = row[col]\n",
        "            if pd.notna(val) and val not in ['Not Found', 'null']:\n",
        "                val_str = str(val)[:100] + \"...\" if len(str(val)) > 100 else str(val)\n",
        "                print(f\"   {col}: {val_str}\")\n",
        "\n",
        "    print(f\"\\n📊 Column Value Distributions:\")\n",
        "    for col in df.columns:\n",
        "        if col in ['Code1', 'Unit Test (.cpp file)', 'Category']:\n",
        "            print(f\"\\n{col}:\")\n",
        "            value_counts = df[col].value_counts().head(5)\n",
        "            for val, count in value_counts.items():\n",
        "                val_str = str(val)[:50] + \"...\" if len(str(val)) > 50 else str(val)\n",
        "                print(f\"   '{val_str}': {count} times\")\n",
        "\n",
        "def inspect_sample_entries(df, n=3):\n",
        "    \"\"\"\n",
        "    Inspect a few sample entries to see the cleaning results\n",
        "    \"\"\"\n",
        "    print(f\"\\n🔍 SAMPLE ENTRIES (showing {n} examples)\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    for i in range(min(n, len(df))):\n",
        "        print(f\"\\n--- ENTRY {i+1} ---\")\n",
        "        row = df.iloc[i]\n",
        "\n",
        "        print(f\"File: {row['Base File Name']}\")\n",
        "        print(f\"Category: {row['Category']}\")\n",
        "\n",
        "        # Show Code1 content (truncated) - UPDATED COLUMN NAME\n",
        "        if pd.notna(row['Code1']) and row['Code1'] not in ['Not Found', 'null', '']:\n",
        "            code1_content = str(row['Code1'])[:200] + \"...\" if len(str(row['Code1'])) > 200 else str(row['Code1'])\n",
        "            print(f\"Code1 Content: {code1_content}\")\n",
        "\n",
        "        # Show Unit Test content (truncated)\n",
        "        if pd.notna(row['Unit Test (.cpp file)']) and row['Unit Test (.cpp file)'] not in ['Not Found', 'null', '']:\n",
        "            test_content = str(row['Unit Test (.cpp file)'])[:200] + \"...\" if len(str(row['Unit Test (.cpp file)'])) > 200 else str(row['Unit Test (.cpp file)'])\n",
        "            print(f\"Unit Test Content: {test_content}\")\n",
        "\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "def export_by_category(df, output_dir=\"cleaned_by_category\"):\n",
        "    \"\"\"\n",
        "    Export cleaned data separated by category\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    if 'Category' in df.columns:\n",
        "        for category in df['Category'].unique():\n",
        "            if pd.notna(category):\n",
        "                category_df = df[df['Category'] == category]\n",
        "                filename = f\"{output_dir}/{category.replace('/', '_').replace(' ', '_')}.csv\"\n",
        "                category_df.to_csv(filename, index=False)\n",
        "                print(f\"📁 Exported {len(category_df)} rows to {filename}\")\n",
        "\n",
        "\n",
        "# Usage Example\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the cleaning process\n",
        "    \"\"\"\n",
        "    print(\"🔧 EMBEDDED UNIT TEST DATASET CLEANER\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Initialize cleaner\n",
        "    cleaner = EmbeddedDatasetCleaner()\n",
        "\n",
        "    # Load and explore dataset first\n",
        "    df = cleaner.load_dataset()\n",
        "\n",
        "    # Debug: Show dataset structure\n",
        "    print(f\"\\n🔍 DATASET EXPLORATION\")\n",
        "    print(\"=\" * 30)\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "\n",
        "    # Show sample of non-null values in key columns\n",
        "    for col in ['Code1', 'Unit Test (.cpp file)', 'Category']:\n",
        "        if col in df.columns:\n",
        "            non_null_count = df[col].notna().sum()\n",
        "            not_found_count = (df[col] == 'Not Found').sum()\n",
        "            null_count = (df[col] == 'null').sum()\n",
        "            empty_count = (df[col] == '').sum()\n",
        "\n",
        "            print(f\"\\n📊 {col}:\")\n",
        "            print(f\"   Non-null: {non_null_count}\")\n",
        "            print(f\"   'Not Found': {not_found_count}\")\n",
        "            print(f\"   'null': {null_count}\")\n",
        "            print(f\"   Empty string: {empty_count}\")\n",
        "\n",
        "            # Show a few sample values\n",
        "            sample_values = df[col].dropna().head(3).tolist()\n",
        "            print(f\"   Sample values: {sample_values}\")\n",
        "\n",
        "    # Ask user if they want to proceed with gentle or aggressive cleaning\n",
        "    print(f\"\\n🎯 CLEANING OPTIONS:\")\n",
        "    print(\"1. Gentle cleaning (preserve more data)\")\n",
        "    print(\"2. Aggressive cleaning (remove more noise)\")\n",
        "\n",
        "    # For now, let's use gentle cleaning by default\n",
        "    print(\"Using gentle cleaning...\")\n",
        "\n",
        "    # Examine data before cleaning\n",
        "    examine_dataset_structure(df)\n",
        "\n",
        "    # Clean the dataset with modified approach\n",
        "    cleaned_df = cleaner.clean_dataset_gentle()\n",
        "\n",
        "    if len(cleaned_df) > 0:\n",
        "        # Analyze results\n",
        "        cleaner.analyze_cleaned_data()\n",
        "\n",
        "        # Create visualizations\n",
        "        cleaner.create_visualizations()\n",
        "\n",
        "        # Save results\n",
        "        cleaner.save_cleaned_data()\n",
        "\n",
        "        print(\"\\n🎉 All done! Your dataset has been cleaned and saved.\")\n",
        "    else:\n",
        "        print(\"\\n❌ No data remaining after cleaning. Try gentler settings.\")\n",
        "        # Show original data for inspection\n",
        "        inspect_sample_entries(df, n=5)\n",
        "\n",
        "    return cleaned_df\n",
        "\n",
        "# Run the cleaning process\n",
        "if __name__ == \"__main__\":\n",
        "    cleaned_dataset = main()\n",
        "\n",
        "# Print instructions for further use\n",
        "print(\"\"\"\n",
        "🎯 NEXT STEPS:\n",
        "\n",
        "1. Inspect sample entries:\n",
        "   inspect_sample_entries(cleaned_dataset, n=5)\n",
        "\n",
        "2. Export by category:\n",
        "   export_by_category(cleaned_dataset)\n",
        "\n",
        "3. Further analysis:\n",
        "   # Count non-null entries per column\n",
        "   cleaned_dataset.count()\n",
        "\n",
        "   # Check specific categories\n",
        "   cleaned_dataset[cleaned_dataset['Category'] == 'Test Module']\n",
        "\n",
        "4. Custom filtering:\n",
        "   # Only entries with unit tests\n",
        "   with_tests = cleaned_dataset[cleaned_dataset['Unit Test (.cpp file)'].notna()]\n",
        "\n",
        "   # Only C/H file pairs\n",
        "   pairs = cleaned_dataset[cleaned_dataset['Category'] == 'Source/Header Pair']\n",
        "\n",
        "📝 The cleaned dataset is now ready for machine learning, analysis, or further processing!\n",
        "\"\"\")"
      ]
    }
  ]
}